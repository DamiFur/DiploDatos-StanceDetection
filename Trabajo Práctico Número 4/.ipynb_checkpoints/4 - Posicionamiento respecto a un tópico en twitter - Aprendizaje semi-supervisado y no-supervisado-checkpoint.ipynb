{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente trabajo vamos a usar los datos etiquetados y no etiquetados para elaborar clasificadores semi y no supervisados.\n",
    "\n",
    "Primero vamos a retomar lo que hicimos en el práctico combinado de Analisis y Visualización y Curación y vamos a armar dos grafos distintos con dos criterios distintos. Uno de usuarios según los retweets y otro con un criterio elegido por ustedes.\n",
    "\n",
    "Luego vamos a correr un algoritmo para encontrar comunidades en esos grafos. Las comunidades nos van a permitir dividir el conjunto de entrenamiento en clusters.\n",
    "\n",
    "Una vez que tenemos las comunidades/clusters definidos, debemos asignar cada uno a una clase. Cada elemento perteneciente a una comunidad asociada a una clase será etiquetado como de esa clase. Luego usaremos esos ejemplos para entrenar nuestro clasificador.\n",
    "\n",
    "Vamos a hacer esto para los tres tópicos: aborto, cambio climático y feminismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cobertura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá copien lo que hicieron de cobertura del primer práctico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grafo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comunidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como método para detectar las comunidades vamos a usar el algoritmo de Louvain\n",
    "https://es.wikipedia.org/wiki/M%C3%A9todo_de_Louvain\n",
    "\n",
    "Louvain es un método iterativo que busca armar comunidades en función de maximizar la modularidad. A grandes rasgos, arma comunidades tales que hagan que la mayor cantidad de aristas conecten dos nodos que sean de la misma comunidad. El problema es que la solución ideal a esta modularidad es muy costosa computacionalmente. Por lo tanto, Louvain es una heurística iterativa (en cada iteración se acerca un poco más a una solución mejor sin necesariamente llegar a alcanzar una solución ideal). Al ser un método iterativo no es exacto: correrlo varias veces con los mismos parámetros puede dar distintos resultados. El algoritmo termina cuando logra estabilidad, es decir, un valor de modularidad que considera aceptable.\n",
    "\n",
    "Nosotros vamos a trabajar con un parámetro específico de este algoritmo que es la resolución. La resolución afecta la estabilidad del sistema: a mayor resolución, se necesitan más iteraciones para lograr estabilidad. En particular, lo que a nosotros nos interesa, es que **a mayor resolución, el algoritmo encuentra menos comunidades de mayor tamaño** (se polariza).\n",
    "\n",
    "Lo que nosotros queremos hacer es encontrar el menor valor de la resolución que nos de como resultado únicamente  dos comunidades polares que tengan más de una cierta cantidad de usuarios (definida en la variable users_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/cylouvain/\n",
    "import cylouvain\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Devuelve todas las particiones según la resolución. Si hay una particion ideal\n",
    "# (la menor particion que genere 2 o menos comunidades), la devuelve. Sino devuelve -1\n",
    "def findPartitions(g, user_threshold, resolutions):\n",
    "    resolutions.sort()\n",
    "    best_resolution = -1\n",
    "    found_best = False\n",
    "    for resolution in resolutions:\n",
    "        print(\"resolución {:.2f}\".format(resolution), end=\"\")\n",
    "\n",
    "        partition = cylouvain.best_partition(g, resolution=resolution)\n",
    "\n",
    "        num_partitions = len(set(partition.values()))\n",
    "        counter = Counter(partition.values())\n",
    "\n",
    "        # Aca le está diciendo que cuente cuantas son las que tienen usuarios mayores al threshold\n",
    "        important_partitions = len([_ for x in counter.items() if x[1] > user_threshold])\n",
    "\n",
    "\n",
    "        if important_partitions <= 2 and not found_best:\n",
    "            best_resolution = resolution\n",
    "            found_best = True\n",
    "            \n",
    "        print(\"---> {} particiones ({} con más de {} usuarios)\".format(\n",
    "            num_partitions,\n",
    "            important_partitions,\n",
    "            user_threshold,\n",
    "        ))\n",
    "        #print(\"Tiempo: {:.2f} minutos\".format((end - begin) / 60))\n",
    "        partitions[resolution] = partition\n",
    "    return partitions, best_resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo\n",
    "partitions, resolution = findPartitions(g, 100,  [1.0, 1.5, 2.0, 3.0, 5.0, 6.0, 7.0, 10.0, 15.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aca paso del formato lista de tupla<nodo, Número de comunidad> a un mapa que para cada comunidad tiene la lista de los nodos que pertenecen\n",
    "if resolution != -1:\n",
    "    partition = partitions[resolution]\n",
    "    partition_to_ids = {partition_number:[] for partition_number in range(len(set(partition.values())))}\n",
    "\n",
    "    num_partitions = len(set(partition.values()))\n",
    "    for user_id, user_partition in partition.items():\n",
    "        partition_to_ids[user_partition].append(user_id)\n",
    "else:\n",
    "    print(\"Hay más de dos particiones principales\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prueben distintos valores de user_threshold y distintas resoluciones evaluando cosas como cuantos (y cuales) nodos del grafo (sean los nodos usuarios u otra cosa) les quedan por fuera de las dos comunidades más grandes (no necesariamente es malo que queden muchos nodos afuera si verdaderamente no tienen una postura clara sobre el tópico); que proporcion tienen esas dos comunidades más grandes: ¿son parejas entre sí o no?; en el caso de que los nodos sean usuarios, además de los usuarios, ¿son parejas la cantidad de tweets que postean esos usuarios? ¿Cuántos tweets hay de cada comunidad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armen un script que para cada comunidad muestre datos que permitan evaluar cuan bien se ajusta a la clase a la que la queremos asignar. Algunos ejemplos pueden ser: usuarios con más cantidad de retweets, tweets más retwiteados, tweets de los usuarios con más cantidad de tweets, etc. Tiene que ser algo que de un panorama de qué es esa comunidad pero a su vez sencillo de ver y que convenza a alguien que lo ve desde afuera que usar esas comunidades puede servir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagan una nube de palabras con los tweets de cada comunidad y analicen cuan fácil o difícil es inferir la clase de esa comunidad mirando la nube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decidan una clase para cada comunidad. Justifiquen su decisión planteando cuán ajustada les parece que es la asignación en función de los ejercicios anteriores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacer los ejercicios anteriores para los tres tópicos y comparar los resultados entre sí: ¿hay tópicos para los cuales las comunidades reflejan mejor o peor la polaridad respecto al posicionamiento? ¿Por qué les parece que esto sucede así?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generar un dataset con todos los tweets asociados a cada comunidad, cada uno etiquetado según la comunidad a la que pertenece. Usar esos datasets como entrenamiento de algun clasificador basado en alguno de los tres algoritmos que vieron en el práctico pasado: SVM's, Random Forests o Red Neuronal. Reportar las mismas métricas (Accuracy, F1 micro, macro y average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para uno de los tres tópicos, exploren un enfoque mixto supervisado/semi-supervisado. El enfoque mixto puede ser: o bien un Voting Classifier con dos clasificadores que usen el corpus supervisado y uno basado en comunidades (o uno y dos, como prefieran), o un clasificador entrenado con una mitad de ejemplos tomados del corpus de tweets etiquetados y la otra mitad de comunidades. La idea es que definan ustedes alguna manera de usar todos los datos: los etiquetados y los no etiquetados. Luego comparen los resultados obtenidos con las versiones puramente supervisadas o puramente basadas en comunidades."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
